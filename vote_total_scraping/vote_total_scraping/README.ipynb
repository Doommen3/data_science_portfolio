{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ca3f5f2-7a85-4688-ae1e-72ac13fd3be3",
   "metadata": {},
   "source": [
    "This project required data collection for a political science professor. We collected presidential election data for every state from 2000 to 2020. The data was collected from official house clerk election statistics. To accomplish this I wrote 1) a python program that downloaded all presidential election data pdf's stored on the website, 2) a program that scraped data from the pdf's after identifying relevant pages on the pdf's, and then stored the data in an excel sheet for the professor to use. \n",
    "\n",
    "The download_datafiles.py file downloaded the pdf files from the website and stores them on a local folder. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172a3cb-3406-4901-8755-3e6dc41c844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "def download_pdf(pdf_url, filename):\n",
    "    \"\"\"\n",
    "    Downloads a PDF from the given URL and saves it locally.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading: {pdf_url}\")\n",
    "        response = requests.get(pdf_url)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Saved as: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {pdf_url}: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Base page containing the election statistics links\n",
    "    base_page = \"https://history.house.gov/Institution/Election-Statistics/Election-Statistics/\"\n",
    "    # Define the target presidential election years\n",
    "    target_years = [\"2000\", \"2004\", \"2008\"]\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_page)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching the base page: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Create a directory to store downloaded PDFs\n",
    "    download_dir = \"pdfs\"\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    # Find all anchor tags whose href attribute contains \".pdf\"\n",
    "    pdf_links = soup.find_all(\"a\", href=lambda href: href and \".pdf\" in href.lower())\n",
    "\n",
    "    if not pdf_links:\n",
    "        print(\"No PDF links found on the page!\")\n",
    "        return\n",
    "\n",
    "    for link in pdf_links:\n",
    "        link_text = link.get_text().strip()\n",
    "        href = link.get(\"href\")\n",
    "        # Check if either the link text or URL contains one of the target years.\n",
    "        # Also ensure that the link text mentions \"president\" or \"presidential\".\n",
    "        if (any(year in link_text or year in href for year in target_years) and\n",
    "                (\"president\" in link_text.lower() or \"presidential\" in link_text.lower())):\n",
    "            pdf_url = urljoin(base_page, href)\n",
    "            filename = os.path.join(download_dir, os.path.basename(pdf_url))\n",
    "            download_pdf(pdf_url, filename)\n",
    "        else:\n",
    "            # If needed, uncomment the following line to see which links were skipped.\n",
    "            # print(f\"Skipping link (not matching criteria): {link_text} -> {href}\")\n",
    "            pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b1ae5c-1a12-45c4-b823-6f8b87e91b30",
   "metadata": {},
   "source": [
    "The main_3.py file opens every file and selects pages based on whether a line of text is found that precedes the presidential election information. The program stories the State, party, number of votes, and year for every election year pdf. The program then outputs the file into an excel file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0943f588-f548-4813-8a51-f4f7c1526ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pdfplumber\n",
    "from openpyxl import Workbook\n",
    "\n",
    "def extract_presidential_elector_data(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts the 'For Presidential Electors' section data from a PDF.\n",
    "    Returns a list of tuples (state, party, votes).\n",
    "    \"\"\"\n",
    "    data = []  # holds tuples of (state, party, votes)\n",
    "    # Regex to capture a party name and numeric votes, e.g. \"Republican ... 273,559\"\n",
    "    party_votes_pattern = re.compile(r'^(.*?)\\s+(\\d[\\d,]*)$')\n",
    "\n",
    "    # Regex for headings: e.g. \"ARKANSAS\", \"ARKANSAS—Continued\", \"CALIFORNIA - CONTINUED\"\n",
    "    state_heading_pattern = re.compile(\n",
    "        r'^([A-Z ]+)(?:[—-]\\s*CONTINUED)?$',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        current_state = None\n",
    "        capturing = False\n",
    "\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            lines = text.split('\\n')\n",
    "            for line in lines:\n",
    "                print(\"DEBUG repr(line):\", repr(line))\n",
    "\n",
    "                # Strip possible weird chars and work in uppercase\n",
    "                line_stripped = line.strip().upper()\n",
    "                line_stripped = re.sub(r'[\\u00A0\\x0c]+', '', line_stripped)\n",
    "\n",
    "                # 1) First, check if this is the \"FOR PRESIDENTIAL ELECTORS\" line.\n",
    "                #    This must be checked before the state-heading regex.\n",
    "                if 'FOR PRESIDENTIAL ELECTORS' in line_stripped:\n",
    "                    capturing = True\n",
    "                    continue\n",
    "\n",
    "                # 2) If we are capturing and the line starts with \"FOR \", stop capturing.\n",
    "                if capturing and line_stripped.startswith('FOR '):\n",
    "                    capturing = False\n",
    "                    continue\n",
    "\n",
    "                # 3) Next, check for a state heading.\n",
    "                #    This block is reached only if the line wasn't caught by the above.\n",
    "                match_state = state_heading_pattern.match(line_stripped)\n",
    "                if match_state:\n",
    "                    new_state = match_state.group(1).strip()\n",
    "                    current_state = new_state\n",
    "                    capturing = False  # reset capturing for new state\n",
    "                    continue\n",
    "\n",
    "                # 4) When capturing, match party/vote lines.\n",
    "                if capturing:\n",
    "                    match = party_votes_pattern.match(line.strip())\n",
    "                    if match:\n",
    "                        party_str = match.group(1).strip()\n",
    "                        votes_str = match.group(2).replace(',', '')\n",
    "                        try:\n",
    "                            votes = int(votes_str)\n",
    "                        except ValueError:\n",
    "                            votes = 0\n",
    "                        data.append((current_state, party_str, votes))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_year_from_filename(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts the first 4-digit year (e.g., 2000, 2004, 2008) found in the PDF filename.\n",
    "    Returns the year as a string, or None if not found.\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(pdf_path)\n",
    "    match = re.search(r'(19|20)\\d{2}', basename)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "\n",
    "def save_data_to_excel(data, output_xlsx):\n",
    "    \"\"\"\n",
    "    Saves a list of (state, party, votes, year) tuples to an Excel file.\n",
    "    \"\"\"\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Presidential Electors\"\n",
    "\n",
    "    # Write header row with Year included\n",
    "    ws.append([\"State\", \"Party\", \"Votes\", \"Year\"])\n",
    "\n",
    "    # Write data rows\n",
    "    for row in data:\n",
    "        ws.append(row)\n",
    "\n",
    "    wb.save(output_xlsx)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Folder where PDF files are stored\n",
    "    pdf_folder = \"pdfs\"\n",
    "    # Use glob to find all PDF files in the folder\n",
    "    pdf_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in folder: {pdf_folder}\")\n",
    "        return\n",
    "\n",
    "    all_data = []  # To store data from all PDFs\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing {pdf_file}...\")\n",
    "        year = extract_year_from_filename(pdf_file)\n",
    "        if not year:\n",
    "            print(f\"Year not found in filename: {pdf_file}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Extract the election data from the current PDF\n",
    "        elector_data = extract_presidential_elector_data(pdf_file)\n",
    "        # Append the extracted year to each record\n",
    "        for record in elector_data:\n",
    "            state, party, votes = record\n",
    "            all_data.append((state, party, votes, year))\n",
    "\n",
    "    # Save the combined data to an Excel file\n",
    "    output_excel = \"output_3.xlsx\"\n",
    "    save_data_to_excel(all_data, output_excel)\n",
    "    print(\"Extraction complete. Data saved to\", output_excel)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef8fb8-aa1e-413b-99ab-89851c3f4a18",
   "metadata": {},
   "source": [
    "The clean_data_2.py file cleans the data and outputs it to a separate file. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
